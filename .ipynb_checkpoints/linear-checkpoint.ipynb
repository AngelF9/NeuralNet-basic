{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d0496f-d055-46ec-8c73-1480686e7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79197c-45ec-4759-8479-11971efd8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a36708-b1fc-4824-9f27-064becc44464",
   "metadata": {},
   "source": [
    "### The __init__ Method\n",
    "- The __init__ method is the constructor of the class. It initializes the object's state and sets up the layers and parameters.\n",
    "- super() Function: It returns a temporary object of the superclass (nn.Module) that ***allows you to call its methods.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f199e42c-88ab-4d44-8d58-bfc4ab3f4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        super(NueralNet, self).__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a82a1-1baf-469e-ac1e-e3845c3a1066",
   "metadata": {},
   "source": [
    "### Defining Layer\n",
    "- there are many layers to choose from\n",
    "    - Linear\n",
    "    - Convolutional\n",
    "    - Recurrent\n",
    "    - Pooling\n",
    "    - Activation Functions:\n",
    "        - ReLU (nn.ReLU): Rectified Linear Unit.\n",
    "        - Sigmoid (nn.Sigmoid)\n",
    "        - Tanh (nn.Tanh)\n",
    "        - Leaky ReLU (nn.LeakyReLU)\n",
    "#### Linear Transformation\n",
    "\n",
    "$ y = xW^t + b$\n",
    "\n",
    "- x: input tensor\n",
    "- W: weight matrix (learnable)\n",
    "- b: bias vector (learnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c80cc8e1-3d08-403f-be00-4b4bf44c965c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# setting up layers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "        # setting up layers\n",
    "        self.linear = nn.Linear(1, 1) # in_features, out_feautres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe57021-7fe7-4021-bb62-ffad8500c38e",
   "metadata": {},
   "source": [
    "#### Whats a feature?\n",
    "In a scenerion where we are building a weather Nueral net\n",
    "- Input: humidity, temp, longitude, latitude\n",
    "    - these are features\n",
    "    - in a dataset they are the columns\n",
    "- Output: int or rather rain in inches\n",
    "    - can be found in ***labeled dataset***\n",
    "    - this is also a feature (column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b2482-9481-492f-a933-da7d7014efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "        return self.linear(x) # applies linear layer defined in init function\n",
    "        # outputs result of linear transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6b62c-4c96-4182-a2c5-668d69f7b33d",
   "metadata": {},
   "source": [
    "### Forward Method\n",
    "defines the computation performed at every call of a neural network module.\n",
    "- an instance of nn.Module\n",
    "- when you pass input data to model..PyTorch internally calls the forward method to compute the output\n",
    "- specifies how input data flows through the layers of the network\n",
    "#### how foward can be used\n",
    "output = model(input)\n",
    "- essentially calling model.__call__(input) which calls model.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65ac10-75f9-428e-b3c4-49e1515c9718",
   "metadata": {},
   "source": [
    "### Forward in our code\n",
    "- self refers to the instance of the SimpleNN class\n",
    "- x: the input tensor to the network\n",
    "- self.linear(x): applies the linear transformation defined in:\n",
    "    - self.linear to the input x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7b79a-fb70-49c1-8367-ae602b8649d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8589c-ed4c-49f7-a3b5-d4f36b1b9647",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "- quatifies the error between the output of the algorithm\n",
    "  and given target value\n",
    "- measure how well the model is performing\n",
    "- in training, we want this loss function to minimize by adjusting the models parameters (weights and biases) ***lower loss indicates better performance***\n",
    "\n",
    "#### Mean Sqaure Error Loss\n",
    "- loss function used for regression problems, where the goal is to predict continuous values.\n",
    "\n",
    "$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "$\n",
    "where:\n",
    "\n",
    "- \\( n ) is the number of samples.\n",
    "- ( $y_i $) is the actual value.\n",
    "- ( $\\hat{y}_i $) is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a14bd5-0904-4f8f-a07d-6695a23866ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae60cba-ccf3-4853-b70c-387266f0d33d",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "- updates the models parameters based on the computed gradients to minimize the loss function\n",
    "- in training, it implements the algorithm for updating weights, guiding the model towards better performance.\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "- a first order optimization algo for finding the minimum of a function.\n",
    "- updates parameters in the opposite direction of the gradient of the loss function.\n",
    "- stocahstic: refers to randomness. IN SDG, the gradient is computed using a randomly selected subset of data\n",
    "- benefit of this is that it reduces computation time and can help escape local minima.\n",
    "\n",
    "optim.SDG\n",
    "- creates the SGD optimizer\n",
    "model.parameters()\n",
    "- passes the models parameter to the optimizer so it knows what to update\n",
    "lr=0.01\n",
    "- sets the learning rate. How much to adjust the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4ab6aa9-991a-4f4b-872e-d83ab4e48eff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f63856-f05f-492b-a209-52b87255cb41",
   "metadata": {},
   "source": [
    "### Generating Data - Torch.tensor\n",
    "- a function to create a tensor. A multi-dimensional array.\n",
    "- tensors are the primary data structure used in PyTorch.\n",
    "- Each inner list represent a smaple with signle ***feature***\n",
    "- Shape: resulting tensor has a shape of ***(5,1)***\n",
    "    - meaning 5 samples\n",
    "    - 1 feature per sample\n",
    "requires_grad=False\n",
    "- indicates that gradients dont need to be computed for this tensor.\n",
    "- we don't need to calculate gradients with respect to the input data during training; we only compute gradients for model parameters (weights and biases).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851a0cf-4ec6-431b-b336-c7b8eb9e7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some training data for y = 2*x\n",
    "x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]], requires_grad=False)\n",
    "y_train = 2 * x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd070d-6f41-41fb-87d5-54a52fde0327",
   "metadata": {},
   "source": [
    "### How to Train A Model\n",
    "training a model involves adjusting its parameters(weights and biases) so that it can accurately map input data to the correct output.\n",
    "- Forward pass: compute the output (prediction) of the netowkr given the input data.\n",
    "- Loss Compuation: calculate the loss, which quantifies the difference between predicted output and the actual target.\n",
    "- Backward Pass(Backpropagation): Compute the gradients of the loss with respect to the models parameters.\n",
    "- Parameter Update: adjust the models parameters uisng an optimization algorithm.\n",
    "\n",
    "#### what is an Epoch\n",
    "means that every sample in the training dataset has been seen by the model once during training.\n",
    "\n",
    "#### what is an Criterion\n",
    "A criterion in PyTorch refers to the loss function used to measure how well the model is performing. It's a function that takes the model's outputs and the target values as inputs and returns a scalar value representing the loss.\n",
    "\n",
    "### Gradients\n",
    "#### Gradients\n",
    "tells use what nudges to all the weights and baises cause the fastest change to the value of the cost function\n",
    "- which changes to what weights, matter the most?\n",
    "- Gradients accumulate becuase it allows for flexible training stategies.\n",
    "- if not cleared, gradeitns from previous iterations will intefere with the current update.\n",
    "- Can become large, leading to unstable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b4c98-429d-4baa-b0e3-2d9d1e7626c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train) # passing input data through models forward method\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train) # calculates the difference between the models predictions and actual target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f03f9-64dd-43cd-aab1-a092b029e0bd",
   "metadata": {},
   "source": [
    "- ***Loss Function*** measures how far the models predictions are from the actual target\n",
    "- ***Backward Pass*** loss function gradient (with respect to each model parameter) are computed\n",
    "- ***Parameter Update*** users the computed gradients to update the models parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73428e7b-de3f-4a2a-99b2-82822f4acec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Zero gradients, backward pass, and update weights\n",
    "    optimizer.zero_grad() # clear the gradients of all optimized parameters\n",
    "    # Backward Pass\n",
    "    loss.backward() # calculates the gradients of the loss with respect to each model parameter\n",
    "    # update parameters\n",
    "    optimizer.step() # use gradients to update model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d589ded-2576-40b7-bcfe-34c2d00b56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91e412-58a6-4338-be39-7ef6ad32908f",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d060b-22af-4a2d-938e-933b7bcfbec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "x_test = torch.tensor([[9.0]])\n",
    "y_test = model(x_test)\n",
    "print(f'Prediction for x = 6: y = {y_test.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189533b-616b-4942-b68e-3c5dc10ae7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
